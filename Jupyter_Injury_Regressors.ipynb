{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best Margin (C) for Support Vector Regression\n",
      "Testing Margin Value: 0.0001 / 150 \tFound score: -0.022974697673870947\n",
      "Testing Margin Value: 10.0001 / 150 \tFound score: 0.9212652464941635\n",
      "Testing Margin Value: 20.0001 / 150 \tFound score: 0.929076102029009\n",
      "Testing Margin Value: 30.0001 / 150 \tFound score: 0.9316506374850463\n",
      "Testing Margin Value: 40.0001 / 150 \tFound score: 0.9331513912418058\n",
      "Testing Margin Value: 50.0001 / 150 \tFound score: 0.934452269141221\n",
      "Testing Margin Value: 60.0001 / 150 \tFound score: 0.9358941334401912\n",
      "Testing Margin Value: 70.0001 / 150 \tFound score: 0.9374372119080674\n",
      "Testing Margin Value: 80.0001 / 150 \tFound score: 0.939028474561317\n",
      "Testing Margin Value: 90.0001 / 150 \tFound score: 0.9405446183887353\n",
      "Testing Margin Value: 100.0001 / 150 \tFound score: 0.9421446444306947\n",
      "Testing Margin Value: 110.0001 / 150 \tFound score: 0.9435628843652821\n",
      "Testing Margin Value: 120.0001 / 150 \tFound score: 0.9447632751403641\n",
      "Testing Margin Value: 130.0001 / 150 \tFound score: 0.9457003129463744\n",
      "Testing Margin Value: 140.0001 / 150 \tFound score: 0.9457003129463744\n",
      "best c: 130.0001\n",
      "Finding the best layer size for the MLP Regression\n",
      "Testing Layer size: 13 / 20 \tFound score: 0.9805119930816225\n",
      "Testing Layer size: 14 / 20 \tFound score: 0.9962630167975894\n",
      "Testing Layer size: 15 / 20 \tFound score: 0.988983383379708\n",
      "Testing Layer size: 16 / 20 \tFound score: 0.9891715016191284\n",
      "Testing Layer size: 17 / 20 \tFound score: 0.993687724758706\n",
      "Testing Layer size: 18 / 20 \tFound score: 0.990232578178862\n",
      "Testing Layer size: 19 / 20 \tFound score: 0.9906035968828865\n",
      "Best Hidden Layer Size: 14\n",
      "Finding Best Nearest Neighbors\n",
      "Testing Neighbors: 1 / 10 \tFound score: 0.4637804441260744\n",
      "Testing Neighbors: 2 / 10 \tFound score: 0.4560761401623687\n",
      "Testing Neighbors: 3 / 10 \tFound score: 0.4309087472142628\n",
      "Testing Neighbors: 4 / 10 \tFound score: 0.41023553157831893\n",
      "Testing Neighbors: 5 / 10 \tFound score: 0.3826695319961795\n",
      "Testing Neighbors: 6 / 10 \tFound score: 0.36704862769287894\n",
      "Testing Neighbors: 7 / 10 \tFound score: 0.35673778823850444\n",
      "Testing Neighbors: 8 / 10 \tFound score: 0.34446003648818035\n",
      "Testing Neighbors: 9 / 10 \tFound score: 0.33427204564481694\n",
      "Found best neighbors: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================================================\n",
      "Logestic Regression accuracy: 1.0\n",
      "K-Nearest Kneighbors accuracy: 0.4637804441260744\n",
      "Support Vector Regression accuracy: 0.9457003129463744\n",
      "Multi Layer Perceptron Regression accuracy: 0.9962630167975894\n",
      "Ensemble of all 4 accuracy: 0.9220798127840262\n",
      "===========================================================\n",
      "\n",
      "\n",
      "Matrix for mode: KNN\n",
      "[[1045   85]\n",
      " [   2  107]]\n",
      "Matrix for mode: Log\n",
      "[[1047    0]\n",
      " [   0  192]]\n",
      "Dumped model to: model/model_nb.pkl\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Injury Prediction\n",
    "#\n",
    "\n",
    "###############################################################################\n",
    "def main():\n",
    "    data = load_data()\n",
    "    X_1, X_2, Y_1, Y_2, columns = split_data(data)\n",
    "    best_model = train_clf(X_1, X_2, Y_1, Y_2, columns)\n",
    "    dump_regs(best_model)\n",
    "    return\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Load the Dataset from a CSV file\n",
    "#\n",
    "def load_data():\n",
    "    import pandas as pd\n",
    "    path='injured-and-uninjured.csv'\n",
    "    df=pd.read_csv(path, sep=',', header=0)\n",
    "    data = df.drop(df.columns[0], axis=1)\n",
    "    data = data.to_dict(orient='records')\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Split the imported CSV file into Training and Testing Datasets\n",
    "# \n",
    "def split_data(data):\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from pandas import DataFrame\n",
    "    vec = DictVectorizer()\n",
    "\n",
    "    df_data = vec.fit_transform(data).toarray()\n",
    "    feature_names = vec.get_feature_names()\n",
    "    df_data = DataFrame(\n",
    "    df_data,\n",
    "    columns=feature_names)\n",
    "    #print(feature_names)\n",
    "    outcome_feature = df_data['Injured']\n",
    "    target_features = df_data.drop('Injured', axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_1: independent variables for first data set\n",
    "    Y_1: dependent (target) variable for first data set\n",
    "    X_2: independent variables for the second data set\n",
    "    Y_2: dependent (target) variable for the second data set\n",
    "    \"\"\"\n",
    "    \n",
    "    X_1, X_2, Y_1, Y_2 = train_test_split(\n",
    "            target_features, outcome_feature, test_size=0.5, random_state=0)\n",
    "    \n",
    "    return X_1, X_2, Y_1, Y_2, feature_names\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Trains Each of the classifiers and finds prints their accuracy scores\n",
    "#\n",
    "def train_clf(X_1, X_2, Y_1, Y_2, columns):                       \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.ensemble import VotingRegressor\n",
    "    \n",
    "    # Support Vector Regressor -> find best margin\n",
    "    best_c = find_best_c(X_1, X_2, Y_1, Y_2)\n",
    "    svr_reg = SVR(C=best_c, gamma=\"auto\")\n",
    "    svr_reg.fit(X_1,Y_1)\n",
    "    \n",
    "    # Find the best number of hidden layers for the MLP\n",
    "    best_hidden = find_best_layers(X_1, X_2, Y_1, Y_2)\n",
    "    mlp_reg = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(best_hidden), random_state = 1)\n",
    "    mlp_reg.fit(X_1, Y_1)\n",
    "    \n",
    "    # Find the best number of neighbors for the KNN Regressor\n",
    "    best_neighbors = find_best_number_neighbors(X_1, X_2, Y_1, Y_2)\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors = best_neighbors)\n",
    "    knn_reg.fit(X_1,Y_1)\n",
    "    \n",
    "    # Logistic Regressor\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_1,Y_1)   \n",
    "    \n",
    "    \n",
    "    #ensemble of all previous\n",
    "    voting_reg = VotingRegressor(estimators=[('knn', knn_reg), ('mlp', mlp_reg), ('svr', svr_reg)])\n",
    "    voting_reg.fit(X_1, Y_1)\n",
    "    \n",
    "    score_log = log_reg.score(X_2, Y_2)\n",
    "    score_knn = knn_reg.score(X_2, Y_2)\n",
    "    score_svr = svr_reg.score(X_2, Y_2)\n",
    "    score_mlp = mlp_reg.score(X_2, Y_2)\n",
    "    score_ensemble = voting_reg.score(X_2, Y_2)\n",
    "    \n",
    "    \n",
    "                \n",
    "    #score_ensemble =\n",
    "    print(\"\\n\\n===========================================================\")\n",
    "    print (\"Logestic Regression accuracy: {0}\".format(score_log.mean()))\n",
    "    print (\"K-Nearest Kneighbors accuracy: {0}\".format(score_knn.mean()))\n",
    "    print (\"Support Vector Regression accuracy: {0}\".format(score_svr.mean()))\n",
    "    print (\"Multi Layer Perceptron Regression accuracy: {0}\".format(score_mlp.mean()))\n",
    "    print (\"Ensemble of all 4 accuracy: {0}\".format(score_ensemble.mean()))\n",
    "    print(\"===========================================================\\n\\n\")\n",
    "    \n",
    "    print_matrix(knn_reg, X_2, Y_2, 'KNN')\n",
    "    print_matrix(log_reg, X_2, Y_2, 'Log')\n",
    "    #print_matrix(svr_reg, X_2, Y_2, 'SVR')\n",
    "    #print_matrix(mlp_reg, X_2, Y_2, 'MLP')\n",
    "    #print_matrix(voting_reg, X_2, Y_2, 'Ensemble')\n",
    "    \n",
    "    #scores = [log_reg.score(X_2, Y_2), knn_reg.score(X_2, Y_2), svr_reg.score(X_2, Y_2), mlp_reg.score(X_2, Y_2), voting_reg.score(X_2, Y_2)]\n",
    "    #models = [log_reg, knn_reg, svr_reg, mlp_reg, voting_reg]\n",
    "    return svr_reg\n",
    "###############################################################################\n",
    "#\n",
    "# Finds the best Single Layer Size for a Multi Layer Perceptron Regressor\n",
    "#\n",
    "def find_best_layers(X_1, X_2, Y_1, Y_2):\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    print(\"Finding the best layer size for the MLP Regression\")\n",
    "    layer_size = 13\n",
    "    best_score = 0\n",
    "    best_hidden = layer_size\n",
    "    TOTAL_TEST_SIZE = 20\n",
    "    while (layer_size < TOTAL_TEST_SIZE):\n",
    "        nn_clf = MLPRegressor(solver='lbfgs', hidden_layer_sizes=(layer_size), random_state = 1)\n",
    "        nn_clf.fit(X_1, Y_1)   \n",
    "        score = nn_clf.score(X_2, Y_2, sample_weight=None)\n",
    "        print(\"Testing Layer size: \" + str(layer_size), \"/\", str(TOTAL_TEST_SIZE), \"\\tFound score:\", str(score))\n",
    "        \n",
    "        if (score > best_score and score != 1.0):\n",
    "            best_score = score\n",
    "            best_hidden = layer_size\n",
    "            \n",
    "        layer_size += 1\n",
    "    print(\"Best Hidden Layer Size: \" + str(best_hidden))\n",
    "    return best_hidden\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Finds the best Margin for the Support Vector Regressor by performing grid search\n",
    "#\n",
    "def find_best_c(X_1, X_2, Y_1, Y_2):\n",
    "    from sklearn.svm import SVR\n",
    "    print(\"Finding best Margin (C) for Support Vector Regression\")\n",
    "    c = 0.0001\n",
    "    best_c = c\n",
    "    best_score = 0\n",
    "    TOTAL_TEST_SIZE = 150\n",
    "    while (c < TOTAL_TEST_SIZE):\n",
    "        svr_reg = SVR(C=c, gamma=\"auto\")\n",
    "        svr_reg.fit(X_1,Y_1)\n",
    "        \n",
    "        score_svr = svr_reg.score(X_2, Y_2)\n",
    "        print(\"Testing Margin Value: \" + str(c), \"/\", str(TOTAL_TEST_SIZE), \"\\tFound score:\", str(score_svr))\n",
    "        if (score_svr > best_score):\n",
    "            best_score = score_svr\n",
    "            best_c = c\n",
    "        c += 10\n",
    "        \n",
    "    print(\"best c: \" + str(best_c))\n",
    "    return best_c\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Finds the best number of neighbors for a K nearest neighbors Regressor\n",
    "#\n",
    "def find_best_number_neighbors(X_1, X_2, Y_1, Y_2):\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    print(\"Finding Best Nearest Neighbors\")\n",
    "    best_num_neighbors = 1\n",
    "    best_score = 0\n",
    "    i = 1\n",
    "    TOTAL_TEST_SIZE = 10\n",
    "    while (i < TOTAL_TEST_SIZE):\n",
    "        \n",
    "        knn_reg = KNeighborsRegressor(n_neighbors= i)\n",
    "        knn_reg.fit(X_1,Y_1)\n",
    "        score_knn = knn_reg.score(X_2, Y_2)\n",
    "        print(\"Testing Neighbors: \" + str(i), \"/\", str(TOTAL_TEST_SIZE), \"\\tFound score:\", str(score_knn))\n",
    "        if (score_knn > best_score):\n",
    "            best_score = score_knn\n",
    "            best_num_neighbors = i\n",
    "        i += 1\n",
    "    \n",
    "    print(\"Found best neighbors: \" + str(best_num_neighbors))\n",
    "    return best_num_neighbors\n",
    "\n",
    "###############################################################################\n",
    "#\n",
    "# Prints the confusion Matrix for a given model\n",
    "#\n",
    "def print_matrix(model, X_2, Y_2, model_name):\n",
    "    print(\"Matrix for mode: \" + model_name)\n",
    "    output = model.predict(X_2)\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    matrix = confusion_matrix(output, Y_2)\n",
    "    print (matrix)\n",
    "    return\n",
    "    \n",
    "###############################################################################\n",
    "#\n",
    "# Dumps the Classifiors into a file\n",
    "#\n",
    "def dump_regs(best_model):\n",
    "    FILE_DUMP = 'model/model_nb.pkl'\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(best_model, FILE_DUMP)\n",
    "    print(\"Dumped model to: \" + FILE_DUMP)\n",
    "    return\n",
    "\n",
    "###############################################################################\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
